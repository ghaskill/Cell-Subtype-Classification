{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"execution":{"iopub.execute_input":"2022-04-18T15:49:25.066979Z","iopub.status.busy":"2022-04-18T15:49:25.066324Z","iopub.status.idle":"2022-04-18T15:49:28.872305Z","shell.execute_reply":"2022-04-18T15:49:28.870833Z","shell.execute_reply.started":"2022-04-18T15:49:25.066931Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import *\n","from torch.utils.data import SubsetRandomSampler, Dataset\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","import splitfolders\n","\n","torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-18T15:49:28.874356Z","iopub.status.busy":"2022-04-18T15:49:28.873998Z","iopub.status.idle":"2022-04-18T15:49:29.327295Z","shell.execute_reply":"2022-04-18T15:49:29.326572Z","shell.execute_reply.started":"2022-04-18T15:49:28.874316Z"},"trusted":true},"outputs":[],"source":["absolute_path = os.path.abspath(\"\")\n","relative_path = 'data\\images'\n","PATH = '.\\data\\images'\n","\n","# splitfolders.ratio('E:\\dataset2-master\\dataset2-master\\images\\WBC_DATA', output='E:\\dataset2-master\\dataset2-master\\images', seed=42, ratio=(0.8, 0.1, 0.1), move=False)\n","\n","# pre-normalized transformation\n","transform1 = transforms.Compose([transforms.Resize((120,120))\n","                                , transforms.ToTensor()\n","                                ])\n","\n","train_data = datasets.ImageFolder(PATH + '\\TRAIN', transform=transform1)\n","val_data = datasets.ImageFolder(PATH + '\\VAL', transform=transform1)\n","test_data = datasets.ImageFolder(PATH + '\\TEST', transform=transform1)\n","simple_test_data = datasets.ImageFolder(PATH + '\\TEST_SIMPLE', transform=transform1)\n","\n","# pre-normalized dataloaders\n","original_dataloaders = {\n","    \"train\": torch.utils.data.DataLoader(\n","        train_data\n","    ),\n","    \"validation\": torch.utils.data.DataLoader(\n","        val_data, shuffle=False\n","    ),\n","    \"test\": torch.utils.data.DataLoader(\n","        test_data, batch_size = 128\n","    ),\n","    \"simple test\": torch.utils.data.DataLoader(\n","        simple_test_data\n","    )\n","}\n","\n","def calculate_mean_and_std(loader):\n","    sum, squared_sum, num_batches = 0, 0, 0\n","    for data,_ in loader:\n","        sum += torch.mean(data, dim=[0, 2, 3])\n","        squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n","        num_batches += 1\n","\n","    mean = sum / num_batches\n","    std = (squared_sum / num_batches - mean**2)**0.5\n","\n","    return mean, std\n","\n","mean,std = calculate_mean_and_std(original_dataloaders.get('train'))\n","print(mean)\n","print(std)\n","\n","# normalized transformation\n","norm_transform = transforms.Compose([transforms.Resize((120,120))\n","                                , transforms.ToTensor()\n","                                , transforms.Normalize((mean), (std))\n","                                ])\n","\n","train_data = datasets.ImageFolder(PATH + '\\TRAIN', transform=norm_transform)\n","val_data = datasets.ImageFolder(PATH + '\\VAL', transform=norm_transform)\n","test_data = datasets.ImageFolder(PATH + '\\TEST', transform=norm_transform)\n","simple_test_data = datasets.ImageFolder(PATH + '\\TEST_SIMPLE', transform=norm_transform)\n","\n","# normalized dataloaders\n","dataloaders = {\n","    \"train\": torch.utils.data.DataLoader(\n","        train_data, batch_size = 128, shuffle=True, num_workers=4, pin_memory=True\n","    ),\n","    \"validation\": torch.utils.data.DataLoader(\n","        val_data, batch_size = 128, shuffle=False, num_workers=4, pin_memory=True\n","    ),\n","    \"test\": torch.utils.data.DataLoader(\n","        test_data, batch_size = 128\n","    ),\n","    \"simple test\": torch.utils.data.DataLoader(\n","        simple_test_data\n","    )\n","}\n","\n","train_count = len(train_data)\n","validation_count = len(val_data)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# simple cnn\n","simple_cnn = nn.Sequential(\n","    nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1)\n","    , nn.ReLU()\n","    , nn.Flatten()\n","    , nn.Linear(in_features=27144, out_features=500)\n","    , nn.Linear(in_features=500, out_features=4)\n","    , nn.LogSoftmax(dim=1)\n",")\n","\n","# dropout nn\n","network1 = nn.Sequential(\n","\n","    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n","    , nn.Dropout(0.2)\n","    , nn.ReLU()\n","    , nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1)\n","    , nn.Dropout(0.2)\n","    , nn.ReLU()\n","    , nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    , nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n","    , nn.Dropout(0.2)\n","    , nn.ReLU()\n","    , nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n","    , nn.Dropout(0.2)\n","    , nn.ReLU()\n","    , nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    , nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","    , nn.Dropout(0.2)\n","    , nn.ReLU()\n","    , nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n","    , nn.Dropout(0.2)\n","    , nn.ReLU()\n","    , nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    , nn.Flatten()\n","    , nn.Linear(in_features=19200, out_features=3000)\n","    , nn.Dropout(0.2)\n","    , nn.Linear(in_features=3000, out_features=600)\n","    , nn.Dropout(0.2)\n","    , nn.Linear(in_features=600, out_features=32)\n","    , nn.Dropout(0.2)\n","    , nn.Linear(in_features=32, out_features=4)\n",")\n","\n","# delayed dropout\n","network2 = nn.Sequential(\n","\n","    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n","    , nn.ReLU()\n","    , nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1)\n","    , nn.ReLU()\n","    , nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    , nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n","    , nn.ReLU()\n","    , nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n","    , nn.ReLU()\n","    , nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    , nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","    , nn.ReLU()\n","    , nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n","    , nn.ReLU()\n","    , nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    , nn.Flatten()\n","    , nn.Linear(in_features=19200, out_features=3000)\n","    , nn.Dropout(0.2)\n","    , nn.Linear(in_features=3000, out_features=600)\n","    , nn.Dropout(0.2)\n","    , nn.Linear(in_features=600, out_features=32)\n","    , nn.Dropout(0.2)\n","    , nn.Linear(in_features=32, out_features=4)\n",")\n","\n","# batch normalization cnn 2\n","network3 = nn.Sequential(\n","\n","    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n","    , nn.BatchNorm2d(32)\n","    , nn.ReLU()\n","    , nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n","    , nn.BatchNorm2d(32)\n","    , nn.ReLU()\n","    , nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    , nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n","    , nn.BatchNorm2d(64)\n","    , nn.ReLU()\n","    , nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n","    , nn.BatchNorm2d(64)\n","    , nn.ReLU()\n","    , nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    #, nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n","    #, nn.ReLU()\n","    #, nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n","    #, nn.ReLU()\n","    #, nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    , nn.Flatten()\n","    , nn.Linear(in_features=19200, out_features=3000)\n","    , nn.Dropout(0.5)\n","    , nn.Linear(in_features=3000, out_features=600)\n","    , nn.Dropout(0.5)\n","    , nn.Linear(in_features=600, out_features=32)\n","    , nn.Dropout(0.5)\n","    , nn.Linear(in_features=32, out_features=4)\n","    , nn.LogSoftmax(dim=1)\n",")\n","\n","# larger kernel nn\n","network4 = nn.Sequential(\n","\n","    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=4)\n","    , nn.ReLU()\n","    , nn.Conv2d(in_channels=16, out_channels=16, kernel_size=5, stride=1, padding=4)\n","    , nn.ReLU()\n","    , nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    , nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=4)\n","    , nn.ReLU()\n","    , nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=4)\n","    , nn.ReLU()\n","    , nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    , nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=4)\n","    , nn.ReLU()\n","    , nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, stride=1, padding=4)\n","    , nn.ReLU()\n","    , nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","    , nn.Flatten()\n","    , nn.Linear(in_features=38016, out_features=3000)\n","    , nn.Dropout(0.2)\n","    , nn.ReLU()\n","    , nn.Linear(in_features=3000, out_features=600)\n","    , nn.Dropout(0.2)\n","    , nn.ReLU()\n","    , nn.Linear(in_features=600, out_features=32)\n","    , nn.Dropout(0.2)\n","    , nn.ReLU()\n","    , nn.Linear(in_features=32, out_features=4)\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["\n","class Run_Network:\n","\n","    def __init__(self, name: str, network, learning_rate, epochs):\n","        self.name = name\n","        self.network = network\n","        self.learning_rate = learning_rate\n","\n","        self.epochs = epochs\n","        self.loss_values = []\n","        self.valid_loss_values = []\n","        self.train_acc = []\n","        self.val_acc = []\n","\n","    def train_network(self):\n","        self.network.to(device)\n","\n","        #define optimizer and scheduler and cost function\n","        optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate)\n","        scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=20)\n","        criterion = nn.CrossEntropyLoss()\n","\n","        for epoch in range(self.epochs):  # loop over the dataset multiple times\n","            self.network.train()\n","\n","            tr_correct = 0\n","            tr_total = 0\n","            running_loss = 0.0\n","            \n","            for i, data in enumerate(dataloaders.get('train'), 0):\n","                # get the inputs; data is a list of [inputs, labels]\n","                inputs, labels = data[0].to(device), data[1].to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward + backward + optimize\n","                outputs = self.network(inputs)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                #training loss and accuracy\n","                running_loss += loss.item() * inputs.size(0)\n","                _, predicted = torch.max(outputs.data, 1)\n","                tr_total += labels.size(0)\n","                tr_correct += (predicted == labels).sum().item()\n","\n","            scheduler.step()    \n","            \n","            self.train_acc.append(100 * tr_correct / tr_total)   \n","            self.loss_values.append(running_loss / train_count)\n","            \n","            rounded_accuracy = format(100 * tr_correct / tr_total, '.2f')\n","            self.train_acc.append(100 * tr_correct / tr_total)   \n","            self.loss_values.append(running_loss / train_count)\n","            \n","            print(f'VAL LOSS: {loss} -- VAL ACCURACY: {rounded_accuracy} %')\n","                \n","            correct = 0\n","            total = 0\n","            running_valid_loss = 0.0\n","\n","            # since we're not training, we don't need to calculate the gradients for our outputs\n","            with torch.no_grad():\n","                self.network.eval()\n","\n","                for data in dataloaders.get('validation'):\n","                    images, labels = data[0].to(device), data[1].to(device)\n","                    # calculate outputs by running images through the network\n","                    outputs = self.network(images)\n","                    # the class with the highest energy is what we choose as prediction\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    total += labels.size(0)\n","                    correct += (predicted == labels).sum().item()\n","                    valid_loss = criterion(outputs, labels)\n","                    running_valid_loss += valid_loss.item() * images.size(0)\n","            \n","            rounded_accuracy = format(100 * correct / total, '.2f')        \n","            self.val_acc.append(rounded_accuracy)\n","            self.valid_loss_values.append(running_valid_loss / validation_count)\n","        \n","            print(f'VAL LOSS: {valid_loss} -- VAL ACCURACY: {rounded_accuracy} % -- EPOCH: {epoch}')\n","\n","    def test_network(self):\n","        self.network.to(device)\n","        self.network.eval()\n","        \n","        criterion = nn.CrossEntropyLoss()\n","        correct = 0\n","        total = 0\n","        running_valid_loss = 0.0\n","\n","        with torch.no_grad():\n","            for data in dataloaders.get('simple test'):\n","                images, labels = data[0].to(device), data[1].to(device)\n","                # calculate outputs by running images through the network\n","                outputs = self.network(images)\n","                # the class with the highest energy is what we choose as prediction\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","                valid_loss = criterion(outputs, labels)\n","                running_valid_loss += valid_loss.item() * images.size(0)\n","            \n","            rounded_accuracy = format(100 * correct / total, '.2f')\n","            self.val_acc.append(rounded_accuracy)\n","            self.valid_loss_values.append(running_valid_loss / validation_count)\n","        \n","        print(f'TEST LOSS: {valid_loss} -- TEST ACCURACY: {rounded_accuracy} %')\n","\n","    def plot_stats(self, save_plot=False):\n","        plt.plot(self.loss_values, label='train loss')\n","        plt.plot(self.valid_loss_values, label='valid loss')\n","        plt.legend()\n","        plt.title(f'{self.name} Loss')\n","        if save_plot==True:\n","            plt.savefig(f'.\\model_performance_data\\{self.name}_Loss.png')\n","        plt.show()\n","\n","        plt.plot(self.train_acc, label='train acc')\n","        plt.plot(self.val_acc, label='valid acc')\n","        plt.legend()\n","        plt.title(f'{self.name} Accuracy')\n","        if save_plot==True:\n","            plt.savefig(f'.\\model_performance_data\\{self.name}_Acc.png')\n","        plt.show()\n","\n","    def get_confusion_matrix(self, dataloader='validation', save_plot=False):\n","        self.network.eval()\n","        y_pred = []\n","        y_true = []\n","        \n","        with torch.no_grad():\n","            for data in dataloaders.get(dataloader):\n","                images, labels = data[0].to(device), data[1].to(device)\n","                # calculate outputs by running images through the network\n","                output = self.network(images)\n","                \n","                output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n","                y_pred.extend(output)\n","                \n","                labels = labels.data.cpu().numpy()\n","                y_true.extend(labels)\n","                \n","                classes = ('EOSINOPHIL','LYMPHOCYTE','MONOCYTE','NEUTROPHIL')\n","            \n","            cfm = confusion_matrix(y_true, y_pred)\n","            df_cfm = pd.DataFrame(cfm/sum(cfm), index = [i for i in classes], columns = [i for i in classes])\n","\n","            cm = sns.heatmap(df_cfm, annot = True, cmap = 'Greens')\n","            plt.title(self.name)\n","            if save_plot==True:\n","                plt.savefig(f'.\\model_performance_data\\{self.name}_CM.png')\n","\n","\n","    def clear_loss_and_accuracy(self):\n","        self.loss_values.clear()\n","        self.valid_loss_values.clear()\n","        self.train_acc.clear()\n","        self.val_acc.clear()\n","    \n","    def save_model(self):\n","        torch.save(self.network.state_dict(), f'.\\model_states\\{self.name}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","best_net = Run_Network(\n","    name=\"network1\"\n","    , network=network1\n","    , learning_rate=0.0001\n","    , epochs=100\n",")\n","\n","dropout_and_batch_norm = Run_Network(\n","    name=\"dropout + batch norm\"\n","    , network=network2\n","    , learning_rate=0.0001\n","    , epochs=100\n",")\n","\n","large_first_kernel = Run_Network(\n","    name=\"5x5 Kernel\"\n","    , network=network4\n","    , learning_rate=0.0001\n","    , epochs=100\n",")\n","\n","large_first_kernel.train_network()\n","large_first_kernel.plot_stats()\n","large_first_kernel.get_confusion_matrix()\n","#large_first_kernel.save_model()\n","large_first_kernel.test_network()\n","large_first_kernel.get_confusion_matrix(dataloader='test')\n"]},{"cell_type":"markdown","metadata":{"trusted":true},"source":["print(df_cfm)\n","print(cfm)\n","print(sum(cfm))\n","print(cfm/sum(cfm) * 100)\n","print(sum(cfm/sum(cfm) * 100))"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.7 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"ff9abce6264b36e26b63b5a6357065c32e624c53848f5cb8e38c26a68e240c90"}}},"nbformat":4,"nbformat_minor":4}
